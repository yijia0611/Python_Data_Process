{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5讲 网络爬虫进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >郭峰（Email：guofengsfi@163.com)   \n",
    "副教授、博士生导师  \n",
    "上海财经大学公共经济与管理学院 </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >本讲目录：  \n",
    "5.1. Xpath语法  \n",
    "5.2. 异步加载   \n",
    "5.3. 爬虫数据存储  \n",
    "5.4. API调用  \n",
    "5.5. 代理IP设置</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >网络爬虫学习的几点建议：  \n",
    "1）上讲内容，重点学习，掌握Python爬虫要旨；本讲内容，快速浏览，实际工作中有需要，再专门去学习，没必要为学习而学习；  \n",
    "2）善于使用网络资源，常规网站，网上很容易找到可以参考的代码；  \n",
    "3）相信专业分工的力量，相信市场经济的价值。</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.   Lxml库与Xpath语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >Lxml库是基于libxml2的XML解析库的Python封装。该模块使用C语言编写，解析速度很快。Lxml库使用Xpath语法解析定位网页数据。Xpath是基于XML的树状结构，提供在数据结构树中找寻节点的能力。</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lxml库的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >安装Lxml库：pip install lxml</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=4>修正HTML代码 </font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "#第六行故意省略</h1>当中的\">\"标签\n",
    "text=\"\"\"\n",
    "<div>\n",
    "   <ul>\n",
    "      <li class=\"red\"><h1>red flowers</h1</li>       \n",
    "      <li class=\"yellow\"><h2>yellow flowers item</h2></li>\n",
    "      <li class=\"white\"><h3>white flowers</h3></li>>\n",
    "      <li class=\"black\"><h4>black flowers</h4</li>\n",
    "      <li class=\"blue\"><h5>blue flowers</h5>        \n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"\n",
    "#第10行故意省略</li>标签\n",
    "\n",
    "html=etree.HTML(text)   #利用etree.HTML进行初始化,etree库把HTML文档解析为Element对象\n",
    "#print(html)\n",
    "result=etree.tostring(html)   #输出解析的文档，Lxml库能够自动修正HTML代码，结果自动补齐省略的标签，而且添加了html和body\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xpath语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >XPath（XML Path Language）是一门在XML文档中查找信息的语言，可用来在XML文档中对元素和属性进行遍历。</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >XPath的节点有7种类型：文档节点，元素节点，属性节点，文本节点，命名空间节点，处理指令节点，注释节点。对于我们需要关注的是前面4个节点。</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >Xpath语法资料1：https://blog.csdn.net/weixin_41601173/article/details/80021977  \n",
    "XPath学习资料2：https://www.jianshu.com/p/a5406c4d2267</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 节点关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=\"\"\"\n",
    "<user_database>\n",
    "\n",
    "<user>  \n",
    "    <name>小明</name>  \n",
    "    <sex>男</sex>  \n",
    "    <age>20</id>  \n",
    "    <gpa>3.5</goal>  \n",
    "</user>  \n",
    "\n",
    "</user_database>\n",
    "\"\"\"\n",
    "\n",
    "#父节点--user元素是name、sex、age及gpa元素的父节点。\n",
    "#子节点--name、sex、age及gpa元素都是user元素的子节点\n",
    "#同胞节点--name、sex、age及gpa元素都是同胞节点\n",
    "#先辈节点--先辈节点指节点的父、父的父节点等，name元素的先辈是user元素和user_database元素\n",
    "#后代节点--user_database的后代是user、name、sex、age及gpa元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 节点选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>XPath使用路径表达式XML文档中选取节点</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >/ 从根节点选取,选择某个标签，也可以多层查找，如/html/body/h2  \n",
    "// 从匹配选择的当前节点选择文档中的节点，获取所有信息，如//p，将p标签的所有信息都提取出来  \n",
    " @xx 获取属性，如@href,@src,@title  \n",
    "text() 获取文本内容  \n",
    "comment() 获取注释</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>Xpath语法中的谓语用来查找某个特定的节点或者包含某个指定值的节点，谓语被嵌在方括号中。</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >/user_database/user[1]  选取属于user_database子元素的第一个user元素  \n",
    "//li[@attribute] 选取所有拥有名为attribute属性的li元素  \n",
    "//li[@attribute='red']  选取所有li元素，且这些元素拥有值为red的attribute属性 </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>获取Xpath方法</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >根据Xpath语法规则手写\n",
    "浏览器复制。对于结构复杂的网站可以选中网页中要爬取的部分，右击网页->->copy---copy Xpath。  \n",
    "详细内容见：https://www.jianshu.com/p/a5406c4d2267 </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xpath实例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    " \n",
    "text = \"\"\"\n",
    "<div class=\"wrapper\">\n",
    "    <i class=\"iconfont icon-back\" id=\"back\"></i>\n",
    "    <a href=\"/\" id=\"channel\">新浪社会</a>\n",
    "    <ul id=\"nav\">\n",
    "        <li><a href=\"http://domestic.firefox.sina.com/\" title=\"国内\">国内</a></li>\n",
    "        <li><a href=\"http://world.firefox.sina.com/\" title=\"国际\">国际</a></li>\n",
    "        <li><a href=\"http://mil.firefox.sina.com/\" title=\"军事\">军事</a></li>\n",
    "        <li><a href=\"http://photo.firefox.sina.com/\" title=\"图片\">图片</a></li>\n",
    "        <li><a href=\"http://society.firefox.sina.com/\" title=\"社会\">社会</a></li>\n",
    "        <li><a href=\"http://ent.firefox.sina.com/\" title=\"娱乐\">娱乐</a></li>\n",
    "        <li><a href=\"http://tech.firefox.sina.com/\" title=\"科技\">科技</a></li>\n",
    "        <li><a href=\"http://sports.firefox.sina.com/\" title=\"体育\">体育</a></li>\n",
    "        <li><a href=\"http://finance.firefox.sina.com/\" title=\"财经\">财经</a></li>\n",
    "        <li><a href=\"http://auto.firefox.sina.com/\" title=\"汽车\">汽车1</a></li>\n",
    "    </ul>\n",
    "    <i class=\"iconfont icon-liebiao\" id=\"menu\"></i>\n",
    "</div>\n",
    "\"\"\"\n",
    " \n",
    "# 创建html对象\n",
    "html = etree.HTML(text)\n",
    " \n",
    "# 获取所有a标签的href内容\n",
    "results = html.xpath('//a/@href')\n",
    "print(results)\n",
    "# 获取所有li标签下的a标签的文本内容\n",
    "results2 = html.xpath('//li/a/text()')\n",
    "print(results2)\n",
    "\n",
    "results3 = html.xpath('//li/a/@title')\n",
    "print(results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xpath爬虫实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >豆瓣网top250电影爬虫：https://movie.douban.com/top250</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>解析网页流程</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >通过Requests库获取网页数据  \n",
    "通过Xpath、正则表达式、BeautifulSoup或其他解析工具对网页进行解析，得到想要的数据或链接   \n",
    "注：当网页结构简单并且避免安装库的话，使用正则表达式比较合适。当数据量大需要追求效益时，可以考虑使用Xpath</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import time\n",
    "\n",
    "pathn= \"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/豆瓣网/\" \n",
    "fn = open(pathn+\"douban.txt\", \"a+\",encoding='utf8')  #统计结果读入到一个txt文件当中\n",
    "fn.write(\"name\"+\"#\"+\"score\"+\"\\n\")\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_movie_url(url):\n",
    "    html = requests.get(url,headers=headers)\n",
    "    selector = etree.HTML(html.text)\n",
    "    print(selector)\n",
    "    movie_hrefs = selector.xpath('//div[@class=\"hd\"]/a/@href')  #通过找到其所在位置而定位\n",
    "    #movie_hrefs = selector.xpath('//div[@class=\"pic\"]/a/@href') #另一个定位\n",
    "    print(\"movie_hrefs:\",movie_hrefs)\n",
    "    for movie_href in movie_hrefs:\n",
    "        get_movie_info(movie_href)\n",
    "\n",
    "def get_movie_info(url):\n",
    "    print(url)\n",
    "    html = requests.get(url,headers=headers)\n",
    "    selector = etree.HTML(html.text)\n",
    "    try:\n",
    "        name = selector.xpath('//*[@id=\"content\"]/h1/span[1]/text()')[0]  #通过/text()可以获取标签中的文字信息；这是一个列表\n",
    "                                #//*[@id=\"content\"]/h1/span[1]\n",
    "        print(name)\n",
    "   \n",
    "        director = selector.xpath('//*[@id=\"info\"]/span[1]/span[2]/a/text()')[0]\n",
    "                                #//*[@id=\"info\"]/span[1]/span[2]/a\n",
    "      \n",
    "        print(director)\n",
    "        actors = selector.xpath('//*[@id=\"info\"]/span[3]/span[2]')[0]\n",
    "        actor = actors.xpath('string(.)')  #嵌套标签的获取\n",
    "        actor=actor[0:100]\n",
    "        style = re.findall('<span property=\"v:genre\">(.*?)</span>',html.text,re.S)[0]\n",
    "        country = re.findall('<span class=\"pl\">制片国家/地区:</span> (.*?)<br/>',html.text,re.S)[0]\n",
    "        release_time = re.findall('上映日期:</span>.*?>(.*?)</span>',html.text,re.S)[0]\n",
    "        print(release_time)\n",
    "        time = re.findall('片长:</span>.*?>(.*?)</span>',html.text,re.S)[0]\n",
    "        #time = selector.xpath('//*[@id=\"info\"]/span[13]/text()')[0]  #如果使用这个，则会存在串行的问题\n",
    "        print(time)\n",
    "        score = selector.xpath('//*[@id=\"interest_sectl\"]/div[1]/div[2]/strong/text()')[0]\n",
    "                               #//*[@id=\"interest_sectl\"]/div[1]/div[2]/strong\n",
    "        print(score)      \n",
    "        fn.write(str(name)+\"#\"+str(score)+\"\\n\")\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "urls = ['https://movie.douban.com/top250?start={}'.format(str(i)) for i in range(0, 25, 25)]\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    get_movie_url(url)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.1：将上述代码全部改成正则表达式的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  异步加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >实际案例：高考数据</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >以“掌上高考”网站讲解这类网站的爬取方法，https://gkcx.eol.cn/</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另外易个例子：\n",
    "#http://tianqi.2345.com/wea_history/71452.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >对于异步加载（AJAX）的网页，数据不在“网页源代码”，网页加载这些数据的过程也称为“逆行工程”。第一步，打开浏览器的开发者工具（部分电脑为F12键），选择Network选项，第二步，刷新或者点击下一页，可以出现相关信息，第三步，在headers部分可以看到请求的url，从中可以找到相关规律，第四步，preview当中可以找到相关数据</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤1：了解网站结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤2：试读取某一页信息\n",
    "from urllib import request, parse\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "url='https://api.eol.cn/gkcx/api/?access_token=&page=2&province_id=13&signsafe=&size=20&uri=apidata/api/gk/score/proprovince&year=2020'\n",
    "req=requests.get(url,headers=head)\n",
    "str_temp=req.content.decode(encoding='utf8',errors='ignore')\n",
    "print(str_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.2：尝试使用正则表达式从上述文本中提取信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤3：上述数据非常结构化，直接转成json格式\n",
    "import json\n",
    "data = json.loads(str_temp)\n",
    "print(data)\n",
    "#print(data['data']['item'][0])\n",
    "#print(data['data']['item'][0]['average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤3：数据保存\n",
    "import csv\n",
    "path= \"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/掌上高考/\" \n",
    "\n",
    "f=open(path+'score_prov'+'.csv','a',encoding='utf-8-sig',newline='') \n",
    "c=csv.writer(f)\n",
    "c.writerow(('local_province_name','year','local_type_name','local_batch_name','score'))  \n",
    "\n",
    "\n",
    "for i in range(0,len(data['data']['item'])):\n",
    "    score=data['data']['item'][i]['average']\n",
    "    local_batch_name=data['data']['item'][i]['local_batch_name']\n",
    "    local_province_name=data['data']['item'][i]['local_province_name']\n",
    "    local_type_name=data['data']['item'][i]['local_type_name']\n",
    "    year=data['data']['item'][i]['year']\n",
    "    print(score,local_batch_name,local_province_name,local_type_name,year)\n",
    "    c.writerow((local_province_name,year,local_type_name,local_batch_name,score))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤5：完整数据爬取\n",
    "from urllib import request, parse\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "path= \"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/掌上高考/\"\n",
    "f=open(path+'score_prov2'+'.csv','a',encoding='utf-8-sig',newline='') \n",
    "c=csv.writer(f)\n",
    "c.writerow(('local_province_name','year','local_type_name','local_batch_name','score'))  \n",
    "\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "#url='https://api.eol.cn/gkcx/api/?access_token=&page=2&province_id=13&signsafe=&size=20&uri=apidata/api/gk/score/proprovince&year=2020'\n",
    "\n",
    "url1='https://api.eol.cn/gkcx/api/?access_token=&'\n",
    "for y in [2018,2019,2020]:\n",
    "    for p in range(0,4):\n",
    "        url=url1+'page='+str(p)+'&province_id=13&signsafe=&size=20&uri=apidata/api/gk/score/proprovince&year='+str(y)\n",
    "        req=requests.get(url,headers=head)\n",
    "        str_temp=req.content.decode(encoding='utf8',errors='ignore')\n",
    "        #print(str_temp)\n",
    "        data = json.loads(str_temp)\n",
    "        #print(data)\n",
    "        #print(data['data']['item'][0])\n",
    "        for i in range(0,len(data['data']['item'])):\n",
    "            score=data['data']['item'][i]['average']\n",
    "            local_batch_name=data['data']['item'][i]['local_batch_name']\n",
    "            local_province_name=data['data']['item'][i]['local_province_name']\n",
    "            local_type_name=data['data']['item'][i]['local_type_name']\n",
    "            year=data['data']['item'][i]['year']\n",
    "            print(score,local_batch_name,local_province_name,local_type_name,year)\n",
    "            c.writerow((local_province_name,year,local_type_name,local_batch_name,score))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.3：在上述代码基础上爬取其他(多个)省份分数线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.4：在上述代码基础上改编爬取该网站其他数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2345天气网解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "url='http://tianqi.2345.com/Pc/GetHistory?areaInfo%5BareaId%5D=71452&areaInfo%5BareaType%5D=2&date%5Byear%5D=2020&date%5Bmonth%5D=10'\n",
    "req=requests.get(url,headers=head)\n",
    "str_temp=req.content.decode(encoding='utf-8',errors='ignore')\n",
    "print(str_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(str_temp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "patten='<td>(.*?)</td>'\n",
    "data_new=re.findall(patten,str(data))\n",
    "print(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date=[data_new[i]  for i in range(0,len(data_new)) if i % 4==0]\n",
    "date=[data_new[i].split()[0] for i in range(0,len(data_new)) if i % 4==0]\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather=[data_new[i]  for i in range(0,len(data_new)) if i % 4==1]\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.5：参考上述方法，爬取天气网更多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 爬虫数据存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫数据存储一般原则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >数据存储基本原则：在正式的、大型的爬虫工作中，数据存储变得很重要；大型文本型数据，例如政府工作报告、上市公司信息披露报告，可以一份报告存储为一个单独的txt文档（注意文档命名）；结构化数据，包括短文本数据，例如中国知网数据、股吧评论数据可以存储为一份csv格式数据，但需要注意数据的追加和覆盖等问题；更大型的爬虫，存储为mysql数据库格式等比较好，数据处理时，可以再转换为csv格式等。</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >mySQL安装：上课之初提供的软件：mysql-installer-community-8.0.12.0</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >然后pip安装：pip3 install PyMySQL，参考资料：https://www.runoob.com/python3/python3-mysql.html</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >mySQL数据库的基本介绍如下：https://www.cnblogs.com/Eva-J/p/5133716.html</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >安装MySQL的可视化操作软件navicat premium：https://www.navicat.com.cn/download/navicat-premium</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >mySQL可视化阅读软件，需要激活：https://www.cnblogs.com/exmyth/p/12411922.html\n",
    "（如果这个链接无法打开，则自行百度“navicat premium 安装与激活”）</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫数据存储实操"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#安徽日报爬虫\n",
    "#http://app.ahrb.com.cn/ahrb/layout/202001/01/node_01.html\n",
    "    \n",
    "import pymysql\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "agents = [\n",
    "    \"Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "    \"Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27\",\n",
    "]\n",
    "\n",
    "cnx = pymysql.connect(user='root', password='你的密码', host='localhost', database='anhui2',charset='utf8mb4')  #填上mysql的密码\n",
    "cur = cnx.cursor()\n",
    "#上述password需要修改为你安装mysql时设置的密码\n",
    "\n",
    "def get_info(url_1,url,year,m,d,paper_name):\n",
    "    UA = random.choice(agents)\n",
    "    header = {'User-Agent': UA}\n",
    "    req = requests.get(url, headers=header)\n",
    "    if req.status_code==200:\n",
    "        string=req.content.decode('utf8')\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    #在每日的日报主页中找到各版的链接关键词“数字”\n",
    "    #http://app.ahrb.com.cn/ahrb/layout/202001/01/node_01.html\n",
    "    url_vers = re.findall(r'<p>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<a href=\"node_(.*?).html\">',string,re.S)  #获取主页各版链接的关键词【这里可能需要修改】\n",
    "    #print(url_vers)     \n",
    "    for url_ver in url_vers: \n",
    "        url2 = url_1+'layout/'+year+m+'/'+d+'/'+'node_'+url_ver+'.html'   #获取主页各版的链接【这里可能需要修改】\n",
    "        print(url2)              \n",
    "        res_ver = requests.get(url2)\n",
    "        string1 = res_ver.content.decode('utf-8')\n",
    "        \n",
    "        #版面:不同报纸的正则表达式可能不同#【这里可能需要修改】\n",
    "        #<em>第01版：</em>\n",
    "        node_id=re.findall('<em>(.*?)：</em>',string1,re.S)\n",
    "        node_name=re.findall('<em>(.*?)：</em>',string1,re.S)\n",
    "        print(node_id,node_name)\n",
    "        \n",
    "        #在每版的链接中找到每个板块的正文和标题 【这里可能需要修改】  \n",
    "        #http://app.ahrb.com.cn/ahrb/content/202002/02/c135081.html\n",
    "        #<h3><a href=\"../../../content/202002/02/c135081.html\" >\n",
    "        url3 = re.findall(r'<h3><a href=\"../../../content/\\d+/\\d+/(.*?).html\"', string1, re.S)\n",
    "        #print(url3)\n",
    "        for urli in url3:\n",
    "            url_fin = url_1+'content/'+year+m+'/'+d+'/'+urli+'.html'  #不同报纸编码格式不同【这里可能需要修改】\n",
    "            #print(url_fin)  #每个板块的链接\n",
    "            req2 = requests.get(url_fin)\n",
    "            #print(req2)\n",
    "            string2 = req2.content.decode('utf-8')\n",
    "            #print(string2)\n",
    "            #报道标题：\n",
    "            #<title>格式符习近平普京互致新年贺电 01-要闻-新华社 格式符</title>\n",
    "            title=re.findall('<title id=\\\"titleId\\\">(.*?)_安徽日报数字报</title>',string2,re.S) #【这里需要修改】\n",
    "            print(title)\n",
    "            if len(title)==0:    #部分标题和内容可能存在空白（被删帖了）\n",
    "                title=['']\n",
    "            content=re.findall(r'<!--enpcontent--><p>\\s*(.*?) </p><!--/enpcontent-->',string2) #【这里需要修改】\n",
    "            #print(content)\n",
    "            if len(content)==0:\n",
    "                content=['']\n",
    "        \n",
    "            date=year+'-'+m+'-'+d\n",
    "            \n",
    "            sql = 'INSERT INTO temp (`paper_name`,`date`, `title`) \\\n",
    "                         VALUES (%(paper_name)s, %(date)s,   %(title)s)'\n",
    "            value = {\n",
    "                'paper_name': paper_name,\n",
    "                'date': date,\n",
    "                'title': title[0]\n",
    "           \n",
    "            }\n",
    "            cur.execute(sql, value)\n",
    "            cnx.commit()\n",
    "\n",
    "#程序入口\n",
    "#http://app.ahrb.com.cn/ahrb/layout/202001/01/node_01.html\n",
    "paper_name='安徽日报' #【这里需要修改】\n",
    "path='D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/'\n",
    "f = open(path+'wrong_date.txt','w+')\n",
    "url_1='http://app.ahrb.com.cn/ahrb/'   #不同地区的日报这里需要改编【这里需要修改】\n",
    "\n",
    "year='2020'\n",
    "# month=['01','02','03','04']\n",
    "# daily=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19'\n",
    "#        ,'20','21','22','23','24','25','26','27','28','29','30','31']\n",
    "month=['04']\n",
    "daily=['30','31']\n",
    "for m in month:\n",
    "    for d in daily:\n",
    "            url=url_1+'layout/'+year+m+'/'+d+'/'+'node_01.html'  #不同报纸编码格式可能不同【这里需要修改】\n",
    "            #print(url)\n",
    "            fun_return=get_info(url_1,url,year,m,d,paper_name)\n",
    "            time.sleep(1)\n",
    "            if fun_return==False:\n",
    "                f.write(paper_name+'\\t'+year+'-'+m+'-'+d)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.6：上述代码只保存了少数报纸名称、日期和标题信息，补充其他信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.7：练习将5.2讲中数据存储方式改编为MySQL存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. API调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >别的网站都在反爬，为爬虫设置障碍，API却为爬虫创造便利，设置接口</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >API定义：API又叫应用程序接口，是一些预先定义的函数，或指软件系统不同组成部分衔接的约定。目的是提供应用程序与开发人员基于某软件或硬件得以访问一组例程的能力，而又无需访问原码，或理解内部工作机制的细节。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >准备工作：1、原始数据csv文件；2、登录网址：http://lbsyun.baidu.com/ 首页点击申请密钥按钮，经过填写个人信息、邮箱注册等，成功之后在开放平台上点击“创建应用”，填写相关信息。提交后，在你创建应用的访问应用（AK）那一栏就是你的密钥。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\">网址1：http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-geocoding  \n",
    "网址2：http://lbsyun.baidu.com/index.php?title=webapi/direction-api-v2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取中文地址的经纬度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#本程序需要修改下面的ak = '你的ak'，才能运行\n",
    "import json\n",
    "from urllib.request import urlopen, quote\n",
    "import requests,csv\n",
    "import pandas as pd #导入这些库后边都要用到\n",
    "from math import *\n",
    "import datetime\n",
    "import time\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "path=\"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/api调用/\"\n",
    "f = open(path+\"nssd_test.csv\",encoding='utf8')\n",
    "nssd= pd.read_csv(f,header=0,sep=',')\n",
    "print(nssd.shape)\n",
    "nssd=nssd[0:10]  #作为试探，只要前10个\n",
    "nssd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://api.map.baidu.com/geocoding/v3/?address=\n",
    "    #北京市海淀区上地十街10号&output=json&ak=您的ak&callback=showLocation //GET请求\n",
    "    \n",
    "def getlnglat(address):#构造经纬度获取函数,在百度Web服务API下的Geocoding API接口来获取你所需要地址的经纬度坐标并转化为json结构的数据\n",
    "    url = 'http://api.map.baidu.com/geocoding/v3/'\n",
    "    output = 'json'\n",
    "    ak = '你的密钥' #服务端和浏览器端都可以，最好浏览器端，这样可以有更多额度\n",
    "    add = quote(address) #由于本文城市变量为中文，为防止乱码，先用quote进行编码\n",
    "    url_new = url + '?' + 'address=' + add  + '&output=' + output + '&ak=' + ak\n",
    "    #print(url_new)\n",
    "    req = urlopen(url_new)#抓取url的内容\n",
    "    res = req.read().decode() #将其他编码的字符串解码成unicode\n",
    "    temp = json.loads(res) #对json数据进行解析，json.loads()函数是将json格式数据转换为字典\n",
    "    return temp\n",
    "\n",
    "nssd['author1_lng']=0.0\n",
    "nssd['author1_lat']=0.0\n",
    "nssd['author2_lng']=0.0\n",
    "nssd['author2_lat']=0.0\n",
    "for i in range(0,len(nssd)):\n",
    "    author1_unit=nssd['author1_unit'][i].strip()  #author1_unit读取出来并清除不需要字符\n",
    "    author2_unit=nssd['author2_unit'][i].strip()\n",
    "    if getlnglat(author1_unit)['status']==0:\n",
    "        nssd['author1_lng'][i] = getlnglat(author1_unit)['result']['location']['lng'] #采用构造的函数来获取经度\n",
    "        nssd['author1_lat'][i] = getlnglat(author1_unit)['result']['location']['lat'] #获取纬度\n",
    "    else:\n",
    "        nssd['author1_lng'][i] = -99\n",
    "        nssd['author1_lat'][i] = -99\n",
    "       \n",
    "    if getlnglat(author2_unit)['status']==0:\n",
    "        nssd['author2_lng'][i] = getlnglat(author2_unit)['result']['location']['lng'] #采用构造的函数来获取经度\n",
    "        nssd['author2_lat'][i] = getlnglat(author2_unit)['result']['location']['lat'] #获取纬度\n",
    "    else:\n",
    "        nssd['author2_lng'][i] =-99\n",
    "        nssd['author2_lat'][i] =-99\n",
    "    print(str(author1_unit)+':'+str(nssd['author1_lng'][i] )+\"  \"+str(nssd['author1_lat'][i]) +\"   \"\n",
    "          +str(author2_unit)+':'+str(nssd['author2_lng'][i] )+\"  \"+str(nssd['author2_lat'][i] ))\n",
    "\n",
    "nssd.to_csv(path+'nssd_含经纬度.csv',encoding='utf8')\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print((endtime - starttime).seconds)\n",
    "nssd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习5.8：上述代码使用到了第六讲的数据存储方式,模仿5.2和5.3的代码，改编数据存储方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据经纬度计算两地“球面”距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #导入这些库后边都要用到\n",
    "from math import *\n",
    "import datetime\n",
    "import time\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "path=\"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/api调用/\"\n",
    "f = open(path+\"nssd_含经纬度.csv\",encoding='utf8')\n",
    "nssd= pd.read_csv(f,header=0,sep=',')\n",
    "print(nssd.shape)\n",
    "#print(nssd.head())\n",
    "\n",
    "def calcDistance(lat_b, lng_b, lat_c, lng_c):#输入两点的经纬度，根据球面距离公式计算两者距离\n",
    "    ra = 6378.140  # 赤道半径 (km)\n",
    "    rb = 6356.755  # 极半径 (km)\n",
    "    flatten = (ra - rb) / ra  # 地球扁率\n",
    "    rad_lat_b = radians(lat_b)#计算弧度\n",
    "    rad_lng_b = radians(lng_b)\n",
    "    rad_lat_c = radians(lat_c)\n",
    "    rad_lng_c = radians(lng_c)\n",
    "    pb = atan(rb / ra * tan(rad_lat_b))#返回反正切弧度值。\n",
    "    pc = atan(rb / ra * tan(rad_lat_c))\n",
    "    xx = acos(sin(pb) * sin(pc) + cos(pb) * cos(pc) * cos(rad_lng_b - rad_lng_c))#返回反余弦弧度值\n",
    "    c1 = (sin(xx) - xx) * (sin(pb) + sin(pc)) ** 2 / cos(xx / 2) ** 2\n",
    "    c2 = (sin(xx) + xx) * (sin(pb) - sin(pc)) ** 2 / sin(xx / 2) ** 2\n",
    "    dr = flatten / 8 * (c1 - c2)\n",
    "    distance = ra * (xx + dr)\n",
    "    return distance\n",
    "\n",
    "#编写函数，可以批量计算距离\n",
    "def earth_dist(x):#calcDistance(lat_b, lng_b, lat_c, lng_c)\n",
    "    dist=calcDistance(x['author1_lat'], x['author1_lng'], x['author2_lat'], x['author2_lng'])\n",
    "    return dist\n",
    "nssd['dist']=nssd.apply(earth_dist, axis=1)\n",
    "\n",
    "nssd.to_csv(path+'nssd_含距离.csv',encoding='utf8')\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print((endtime - starttime).seconds)\n",
    "nssd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取驾车距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#本程序仍然需要修改ak = '你的ak'，才能运行\n",
    "import json\n",
    "from urllib.request import urlopen, quote\n",
    "import requests,csv\n",
    "import pandas as pd #导入这些库后边都要用到\n",
    "from math import *\n",
    "import datetime\n",
    "import time\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "path=\"D:/python/郭峰Python讲义/数据与结果/05网络爬虫进阶/api调用/\"\n",
    "f = open(path+\"nssd_含距离.csv\",encoding='utf8')\n",
    "nssd= pd.read_csv(f,header=0,sep=',')\n",
    "print(nssd.shape)\n",
    "#print(nssd.head())\n",
    "\n",
    "def car_dist(lat_b,lng_b, lat_c, lng_c):\n",
    "    url_car=r'http://api.map.baidu.com/directionlite/v1/driving?output=json'  # 路线规划\n",
    "    cod = r\"&coord_type=bd09ll\"# 声明坐标格式,bd09ll(百度经纬度坐标)\n",
    "    AK = '你的密钥'#仅服务端ak支持\n",
    "    ak_car = r'&ak=' + AK\n",
    "    ori = str(lat_b) + ',' + str(lng_b)  # 起点\n",
    "    des = str(lat_c) + ',' + str(lng_c)   #终点\n",
    "    ori2 = r\"&origin=\" + ori\n",
    "    des2 = r\"&destination=\" + des\n",
    "    tac_car = r'&tactics_incity=0'\n",
    "    # 默认值：0。可选值：0：常规路线,1：不走高速,2：躲避拥堵,3：距离较短\n",
    "    aurl_car = url_car + ori2 + des2 + tac_car +  ak_car\n",
    "    res_car = urlopen(aurl_car) #打开网页\n",
    "    cet_car = res_car.read() #解析内容\n",
    "    res_car.close() #关闭#\n",
    "    result_car = json.loads(cet_car) #json转dict\n",
    "    return result_car\n",
    "\n",
    "nssd['dist_car']=0.0\n",
    "nssd['time_car']=0.0\n",
    "for i in range(0,len(nssd)):\n",
    "    result_car= car_dist(nssd['author1_lat'][i],nssd['author1_lng'][i],nssd['author2_lat'][i],nssd['author2_lng'][i])\n",
    "    status = result_car['status']\n",
    "    #print('状态码', status)\n",
    "    if status == 0: #表示成功\n",
    "        rsls = result_car['result']['routes']\n",
    "        if rsls == []:  # 无方案时状态也为0，但只返回一个空list\n",
    "            #diss_bus = '状态' + str(status) + ' ' + '无公交方案'\n",
    "            nssd['dist_car'][i]=-100\n",
    "            nssd['time_car']=-100\n",
    "        else:\n",
    "            nssd['dist_car'][i] = result_car['result']['routes'][0]['distance']/1000  # 公交路线距离总长(千米)\n",
    "            nssd['time_car'] = result_car['result']['routes'][0]['duration']/3600  # 乘车时间(小时)\n",
    "            #diss_bus = '状态' + str(status) + ' '+ str(m_bus) + ' ' + str(time_bus) \n",
    "    else:\n",
    "        pass\n",
    "    print(nssd['author1_unit'][i],nssd['author2_unit'][i],nssd['dist'][i],\n",
    "          nssd['dist_car'][i])\n",
    "\n",
    "nssd.to_csv(path+'nssd_含驾车距离.csv',encoding='utf8')\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print((endtime - starttime).seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. 代理IP的设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫与反爬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >网站反爬的常见方式：网页加密；限制访问频率</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >例如，中国知网的一篇论文：《测度中国数字普惠金融发展:指数编制与空间特征》\n",
    "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=JJXU202004013&v=vnwv0yeOYjL2Nfbgmte34qGHmAcZlCKJD9jtk81bVQGEDMqDCfsvRLE299oHQtqs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >限制ip访问频率和次数进行反爬；\n",
    "解决措施:构造自己的IP代理池，然后每次访问时随机选择代理</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >付费代理的使用，相对免费代理来说,付费代理的稳定性更高</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >付费代理分为两类：\n",
    "一类提供接口获取海盘代理，按天或者按量收费；\n",
    "一类搭建了代理隧道， 直接设置固定域名代理</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 付费代理的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "url ='http://httpbin.org/get'\n",
    "rip1 = requests.get('放入自己购买的API链接')                                    \n",
    "patip='<proxy>(.*?)</proxy>'  # 这样也可以，这个匹配最常用\n",
    "yc=re.findall(patip, rip1.text)\n",
    "j=random.randint(1,9)\n",
    "ippp=yc[j] \n",
    "proxy={'http':ippp}\n",
    "print(proxy)\n",
    "r1 = requests.get(url,proxies= proxy)  #主要记住这一行命令\n",
    "print(r1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大象代理：http://www.daxiangdaili.com/signin\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def getip():\n",
    "    url=\"http://tpv.daxiangdaili.com/ip/\"\n",
    "    id='557818712549993'  #订单，已经失效\n",
    "    n='500'\n",
    "    #/ip/?tid=订单号&num=1\n",
    "    url_new=url+'?'+'tid='+id+'&'+'num='+n\n",
    "    #print(url_new)\n",
    "    req=urlopen(url_new)\n",
    "    ips=req.read().decode()\n",
    "    ips=ips.split()\n",
    "    return ips\n",
    "ips=getip()\n",
    "print(ips)\n",
    "\n",
    "for url in urls:\n",
    "    j1=random.randint(0,len(ips))\n",
    "    ip1=ips[j1]\n",
    "    proxy1={'http':ip1}\n",
    "    req= requests.get(url,proxies=proxy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>本讲结束</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
