{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4讲 网络爬虫入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >郭峰（Email：guofengsfi@163.com)   \n",
    "副教授、博士生导师  \n",
    "上海财经大学公共经济与管理学院 </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >本讲目录：  \n",
    "4.1. 爬虫基本原理与操作  \n",
    "4.2. 正则表达式1   \n",
    "4.3. 正则表达式2  \n",
    "4.4. 网络爬虫实战案例</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 网络爬虫实战案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 政府工作报告：知县网 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>数据爬取 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#政府工作报告的爬取,知县网\n",
    "from urllib import request, parse\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/知县网/'\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "#读取id\n",
    "for id in range(10100,10110): \n",
    "    url='http://www.ahmhxc.com/gongzuobaogao/'+str(id)+'.html'\n",
    "#        http://www.ahmhxc.com/gongzuobaogao/10101.html       #模仿此公式计算而来\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        string=req.content.decode('gb2312')\n",
    "#        print(string)\n",
    "        title=re.findall('<div class=\"title\"><h1>(.*?)</h1></div>',string) \n",
    "#                  <div class=\"title\"><h1>（河南省）2018年柘城县人民政府工作报告（全文）</h1></div>\n",
    "#        title=re.findall('<title>(.*?) _知县网---最有深度、最有态度、最接地气的县域大数据门户</title>',string)  \n",
    "#    <title>（河南省）2018年柘城县人民政府工作报告（全文）_知县网---最有深度、最有态度、最接地气的县域大数据门户</title>\n",
    "        print(title)\n",
    "        text0=re.findall(' <script src=\"/d/js/acmsd/thea8.js\"></script>(.*?)<div class=\"pagebreak\">',string,re.S)    \n",
    "#                        \n",
    "#           print(text)\n",
    "        contents=title+text0            #这是个列表，每个元素是一串文字，而不是一个\n",
    "        for p in range(2,10):\n",
    "            url='http://www.ahmhxc.com/gongzuobaogao/'+str(id)+'_'+str(p)+'.html'\n",
    "#                http://www.ahmhxc.com/gongzuobaogao/10064_3.html       #模仿此公式计算而来\n",
    "            req=requests.get(url,headers=head)\n",
    "            if req.status_code==200:\n",
    "                resp=request.urlopen(url)  #这是另一种获取网页的方式\n",
    "                html=resp.read()\n",
    "                string=html.decode(encoding='gb2312',errors='ignore') \n",
    "                text=re.findall(' <script src=\"/d/js/acmsd/thea8.js\"></script>(.*?)<div class=\"pagebreak\">',string,re.S) \n",
    "                contents=contents+text\n",
    "            else:\n",
    "                continue\n",
    "        f=open(path+'_'+str(id)+'_'+title[0]+'.txt','w+',encoding='utf8') \n",
    "        for content in contents:\n",
    "            f.write(content+'\\n')\n",
    "        f.close()\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)           #爬取间隔数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>数据整理 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】知县网的文件整理\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/知县网/'\n",
    "pathn='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/知县网_new/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    if '政府工作报告' in file:\n",
    "        f=open(path+str(file),encoding='utf8')\n",
    "        text=f.read()\n",
    "        text=text.replace('<!--/广告位-->','')\n",
    "        text=text.replace('<div>',\"\")\n",
    "        text=text.replace('</div>','')            \n",
    "        text=text.replace('&rdquo;','')\n",
    "        text=text.replace('&nbsp;','')\n",
    "        text=text.replace('&ldquo;','')\n",
    "        text=text.replace('</td>','')\n",
    "        text=text.replace('</tr>','')\n",
    "        text=text.replace('</table>','')\n",
    "        text=text.replace('&mdash;','')\n",
    "        text=text.replace('<br />','')\n",
    "        text=text.replace('</strong>','')\n",
    "        text=text.replace('<strong>','')\n",
    "        text=text.replace('<div class=\"TRS_Editor\">','')\n",
    "        text=text.replace('<p align=\"center\" style=\"MARGIN-BOTTOM: 0px; MARGIN-TOP: 0px\">','')\n",
    "        text=text.replace('</p>','')\n",
    "        text=text.replace('<p>','')\n",
    "        text=text.replace('<!--EpointContent-->','')\n",
    "        text=text.replace('<p style=\"TEXT-ALIGN: center\">','')\n",
    "        text=text.replace('</span>','')\n",
    "        text=text.replace('<p style=\"text-align: justify;\"><span style=\"font-size:14px;\">','')\n",
    "        text=text.replace('<div style=\"text-align: justify;\">','')\n",
    "        text=text.replace('<span style=\"font-size:14px;\">','')\n",
    "        text=text.replace('<p style=\"text-align: justify;\">','')\n",
    "        fn=open(pathn+str(file),'w+',encoding='utf-8')\n",
    "        fn.write(text)\n",
    "        f.close()\n",
    "        fn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>报告重命名 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】知县网报告的重命名\n",
    "#要替换，而不是另存，方便知道哪些没改成功\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/知县网_new/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "\n",
    "i=0\n",
    "for file in files:\n",
    "    i=i+1\n",
    "    olddir=os.path.join(path,file)\n",
    "    filename=os.path.splitext(file)[0]\n",
    "    filetype=os.path.splitext(file)[1]\n",
    "    a=file.find('年')\n",
    "    b=file.find('人民政府工作报告')\n",
    "  \n",
    "    newdir=os.path.join(path,filename[a+1:b]+filename[a-4:a]+filetype);#新的文件路径\n",
    "    try:\n",
    "        os.rename(olddir,newdir)\n",
    "    except: \n",
    "        os.remove(olddir)   #有不少重复的，将重复的删除，但这可能是将所有异常全部删除了？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习4.7：上述代码结果有点小错误，尝试改正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 政府工作报告-礼拜五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.libaiwu.com/tag/zfgzbgs/page/2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>两步爬虫：第一步 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先计算一个工作报告的id\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_id/'\n",
    "\n",
    "head = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}\n",
    "f=open(path+'id.txt','w+',encoding='utf-8') \n",
    "for i in range(2,3):   #到2018年4月22日，一共207页。\n",
    "    url='http://www.libaiwu.com/tag/zfgzbgs/page/'+str(i)+'/'\n",
    "    # https://www.libaiwu.com/tag/zfgzbgs/page/2/\n",
    "    print(url)       \n",
    "    req=requests.get(url,headers=head)\n",
    "    #print(req)\n",
    "    if req.status_code==200:\n",
    "        string=req.content.decode('utf8') \n",
    "        ids=re.findall(' href=\"https://www.libaiwu.com/(\\d+).htm',string) \n",
    "        #  href=\"https://www.libaiwu.com/3235.htm\">\n",
    "        print(ids)\n",
    "        for id in ids:\n",
    "            f.write(id+'\\n')\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)    #爬取间隔数据\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>两步爬虫：正式爬虫 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#政府工作报告正式爬取\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "#读取id\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_id/'\n",
    "path2='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_文章/'\n",
    "\n",
    "ids=open(path+'id.txt','r')\n",
    "iss=ids.read()\n",
    "iss=iss.split('\\n')\n",
    "print(iss[1:10])\n",
    "for id in iss: \n",
    "    url='http://www.libaiwu.com/'+id+'.htm'\n",
    "#        https://www.libaiwu.com/2575.htm       #模仿此公式计算而来，谨慎：部分报告有多页！！！数量较少，没有考虑\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        string=req.content.decode('utf8') \n",
    "#    print(string)\n",
    "        title=re.findall('<title>(.*?) - 礼拜五秘书网</title>',string)  \n",
    "#  <title>沈阳市2013年政府工作报告 - 礼拜五秘书网</title>\n",
    "        print(title)\n",
    "        if '政府工作报告' in title[0]:\n",
    "            stringn=re.sub('<table class=\"wenxue\" width=\"100%\">(.*?)</html>','',string,flags=re.S)\n",
    "#        print(stringn)                                 #这里不删减一些的话，下一句匹配不干净\n",
    "            text=re.findall('<p>(.*?)</p>',stringn,re.S)    \n",
    "#        print(text)\n",
    "            contents=title+text             #这是个列表，每个元素是一串文字，而不是一个\n",
    "            f=open(path2+'_'+str(id)+'_'+title[0]+'.txt','w+',encoding='utf-8') \n",
    "            for content in contents:\n",
    "                f.write(content+'\\n')\n",
    "            f.close()\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)           #爬取间隔数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习4.8：思考、尝试把上述两个代码合并成一个代码（可以参考下文解放日报爬虫代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>数据整理 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#礼拜五秘书网的文件整理\n",
    "import os    \n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_文章/'\n",
    "pathn='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_文章_new/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    if '政府工作报告' in file:\n",
    "        f=open(path+str(file),encoding='utf-8')\n",
    "        text=f.read()\n",
    "        text=text.replace('&nbsp;',' ')\n",
    "        text=text.replace('<b>','')\n",
    "        text=text.replace('</b>','')\n",
    "        text=text.replace('</strong>','')\n",
    "        text=text.replace('<strong>','')\n",
    "        text=text.replace('<a href=\"https://www.libaiwu.com/hwd/\">','')\n",
    "        text=text.replace('</a>','')  \n",
    "        text=text.replace('</br >','')\n",
    "        text=text.replace('<wbr />','')\n",
    "        fn=open(pathn+str(file),'w+',encoding='utf-8')\n",
    "        fn.write(text)\n",
    "        f.close()\n",
    "        fn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>报告重命名 </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】报告重命名\n",
    "#要替换，而不是另存，方便知道哪些没改成功【其实也可以存入另一个文件夹，然后原始的文件删除】\n",
    "import os    \n",
    "import re\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/礼拜五_文章_new/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    olddir=os.path.join(path,file)\n",
    "    filename=os.path.splitext(file)[0]\n",
    "    filetype=os.path.splitext(file)[1]\n",
    "    filen=file.replace(' ','')\n",
    "    a=filen.find('年')\n",
    "    b=filen.find('政府工作报告')\n",
    "  \n",
    "    newdir=os.path.join(path,filename[a+1:b]+filename[a-4:a]+filetype);#新的文件路径\n",
    "    try:\n",
    "        os.rename(olddir,newdir)\n",
    "    except: \n",
    "        os.remove(olddir)         #有不少重复的，将重复的删除，但这可能是将所有异常全部删除了？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解放日报爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.jfdaily.com/journal/2020-01-01/page_01.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "agents = [\n",
    "    \"Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "    \"Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27\",\n",
    "]\n",
    "\n",
    "def get_info(url_1,url,year,m,d,paper_name):\n",
    "    UA = random.choice(agents)\n",
    "    header = {'User-Agent': UA}\n",
    "    req = requests.get(url, headers=header)\n",
    "    if req.status_code==200:\n",
    "        string=req.content.decode('utf8')\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    #在每日的日报主页中找到各版的链接关键词“数字”\n",
    "    #https://www.jfdaily.com/journal/2020-01-01/page_01.htm\n",
    "    url_vers = re.findall('onclick=\\\"getPage\\(\\'(\\d+)\\'\\);\\\">',string)  #获取主页各版链接的关键词【这里可能需要修改】\n",
    "    #print(url_vers)     \n",
    "    for url_ver in url_vers:  \n",
    "        url2 = url_1+year+'-'+m+'-'+d+'/'+'page_'+url_ver+'.htm'   #获取主页各版的链接【这里可能需要修改】\n",
    "        print(url2)              \n",
    "        res_ver = requests.get(url2)\n",
    "        string1 = res_ver.content.decode('utf-8')\n",
    "        \n",
    "        #版面:不同报纸的正则表达式可能不同#【这里可能需要修改】\n",
    "        #下一版</a>...</span>...01版：要闻...</div>...</div>...</td>...<td valign=\"top\">\n",
    "        node_id=re.findall('下一版</a>\\s+</span>\\s+(.*?)\\s+</div>\\s+</div>\\s+</td>\\s+<td valign=\\\"top\\\">',string1)\n",
    "        node_name=re.findall('下一版</a>\\s+</span>\\s+(.*?)\\s+</div>\\s+</div>\\s+</td>\\s+<td valign=\\\"top\\\">',string1)\n",
    "        print(node_id,node_name)\n",
    "        \n",
    "        #在每版的链接中找到每个板块的正文和标题 【这里可能需要修改】  \n",
    "        #https://www.jfdaily.com/journal/2020-01-01/getArticle.htm?id=285454\n",
    "        url3 = re.findall(\"href=\\'getArticle.htm\\?id=(\\d+)\\'>\", string1, re.S)\n",
    "        #print(url3)\n",
    "        article_num=len(url3)\n",
    "        i=1\n",
    "        for urli in url3:\n",
    "            url_fin = url_1+year+'-'+m+'-'+d+'/'+'getArticle.htm?id='+urli  #不同报纸编码格式不同【这里可能需要修改】\n",
    "            #print(url_fin)  #每个板块的链接\n",
    "            req2 = requests.get(url_fin)\n",
    "            #print(req2)\n",
    "            string2 = req2.content.decode('utf-8')\n",
    "            #print(string2)\n",
    "            #报道标题：\n",
    "            #<title>格式符习近平普京互致新年贺电 01-要闻-新华社 格式符</title>\n",
    "            title=re.findall('<title>\\r\\n\\t\\t\\t(.*?)\\r\\n\\t\\t</title>',string2,re.S) #【这里需要修改】\n",
    "            print(title)\n",
    "            if len(title)==0:    #部分标题和内容可能存在空白（被删帖了）\n",
    "                title=['']\n",
    "            content=re.findall(r'<!-- min-height:500px; -->\\s*(.*?)<br/><br/>\\s+</div>',string2,re.S) #【这里需要修改】\n",
    "            #print(content)\n",
    "            if len(content)==0:\n",
    "                content=['']\n",
    "        \n",
    "            date=year+'-'+m+'-'+d\n",
    "            f=open(path+'date'+'_'+year+'_'+m+'_'+d+'_'+str(node_id[0])[0:2]+'_'+ str(i)+'.txt'\n",
    "                   ,'w+',encoding='utf-8') \n",
    "            f.write(title[0]+'\\n')\n",
    "            f.write(paper_name+'\\n')\n",
    "            f.write(year+'_'+m+'_'+d+'_'+str(node_id[0])+'\\n')\n",
    "            f.write(content[0])\n",
    "            f.close()\n",
    "            i=i+1\n",
    "      \n",
    "#程序入口\n",
    "#https://www.jfdaily.com/journal/2020-01-01/page_01.htm\n",
    "paper_name='解放日报' #【这里需要修改】\n",
    "path='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/解放日报/'\n",
    "path2='D:/python/郭峰Python讲义/数据与结果/04网络爬虫入门/'\n",
    "f2 = open(path2+'wrong_date.txt','w+')\n",
    "url_1='https://www.jfdaily.com/journal/'   #不同地区的日报这里需要改编【这里需要修改】\n",
    "\n",
    "#year='2020'\n",
    "#month=['01','02','03','04']\n",
    "#daily=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19'\n",
    "#       ,'20','21','22','23','24','25','26','27','28','29','30','31']\n",
    "\n",
    "year='2020'\n",
    "month=['04']\n",
    "daily=['30']\n",
    "\n",
    "for m in month:\n",
    "    for d in daily:\n",
    "            url=url_1+year+'-'+m+'-'+d+'/'+'page_01.htm'  #不同报纸编码格式可能不同【这里需要修改】\n",
    "            #print(url)\n",
    "            fun_return=get_info(url_1,url,year,m,d,paper_name)\n",
    "            time.sleep(1)\n",
    "            if fun_return==False:\n",
    "                f2.write(paper_name+'\\t'+year+'-'+m+'-'+d)\n",
    "                continue1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"宋体\" >注：如果使用上述代码进行大规模爬虫，不排除出现bug，作为教学材料，变更了存储方式</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习4.9：模仿上述代码，爬取其他省份的党报（可以找翻页风格跟解放日报较为相似的党报）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=3>本讲结束</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
